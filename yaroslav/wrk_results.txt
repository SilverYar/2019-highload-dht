GET service
Running 30s test @ http://localhost:8080
  4 threads and 4 connections
  Thread calibration: mean lat.: 2109.887ms, rate sampling interval: 6303ms
  Thread calibration: mean lat.: 2363.676ms, rate sampling interval: 6791ms
  Thread calibration: mean lat.: 2378.763ms, rate sampling interval: 6946ms
  Thread calibration: mean lat.: 2075.907ms, rate sampling interval: 6168ms
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency     5.63s     2.92s   13.34s    83.00%
    Req/Sec     4.30k     1.96k    6.67k    40.00%
  Latency Distribution (HdrHistogram - Recorded Latency)
 50.000%    4.33s 
 75.000%    5.67s 
 90.000%   11.49s 
 99.000%   13.27s 
 99.900%   13.34s 
 99.990%   13.35s 
 99.999%   13.35s 
100.000%   13.35s 

  Detailed Percentile spectrum:
       Value   Percentile   TotalCount 1/(1-Percentile)

    3260.415     0.000000           36         1.00
    3532.799     0.100000        32446         1.11
...
   13352.959     0.999878       318627      8192.00
   13352.959     1.000000       318627          inf
#[Mean    =     5630.360, StdDeviation   =     2922.631]
#[Max     =    13344.768, Total count    =       318627]
#[Buckets =           27, SubBuckets     =         2048]
----------------------------------------------------------
  518326 requests in 30.00s, 34.11MB read
  Non-2xx or 3xx responses: 518326
Requests/sec:  17278.34
Transfer/sec:      1.14MB

GET Async Service
  4 threads and 4 connections
  Thread calibration: mean lat.: 496.545ms, rate sampling interval: 2027ms
  Thread calibration: mean lat.: 484.539ms, rate sampling interval: 1970ms
  Thread calibration: mean lat.: 839.059ms, rate sampling interval: 3389ms
  Thread calibration: mean lat.: 993.764ms, rate sampling interval: 3991ms
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency     2.56s     1.53s    6.21s    63.22%
    Req/Sec     6.74k     1.11k    8.80k    62.07%
  Latency Distribution (HdrHistogram - Recorded Latency)
 50.000%    2.33s 
 75.000%    2.83s 
 90.000%    5.23s 
 99.000%    6.11s 
 99.900%    6.22s 
 99.990%    6.22s 
 99.999%    6.22s 
100.000%    6.22s 

  Detailed Percentile spectrum:
       Value   Percentile   TotalCount 1/(1-Percentile)

     614.399     0.000000           28         1.00
     852.991     0.100000        53383         1.11
...
    6217.727     0.998633       532205       731.43
    6217.727     1.000000       532205          inf
#[Mean    =     2560.233, StdDeviation   =     1530.324]
#[Max     =     6213.632, Total count    =       532205]
#[Buckets =           27, SubBuckets     =         2048]
----------------------------------------------------------
  791244 requests in 30.00s, 52.07MB read
  Non-2xx or 3xx responses: 791244
Requests/sec:  26375.89
Transfer/sec:      1.74MB

GET sharding
Running 30s test @ http://localhost:8080
  4 threads and 4 connections
  Thread calibration: mean lat.: 2.597ms, rate sampling interval: 10ms
  Thread calibration: mean lat.: 2.040ms, rate sampling interval: 10ms
  Thread calibration: mean lat.: 2.667ms, rate sampling interval: 10ms
  Thread calibration: mean lat.: 1.996ms, rate sampling interval: 10ms
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency     4.51ms   16.44ms 172.67ms   95.87%
    Req/Sec     1.34k   393.37     6.50k    93.57%
  Latency Distribution (HdrHistogram - Recorded Latency)
 50.000%    1.30ms
 75.000%    1.83ms
 90.000%    2.56ms
 99.000%  106.82ms
 99.900%  160.26ms
 99.990%  171.52ms
 99.999%  172.29ms
100.000%  172.80ms

  Detailed Percentile spectrum:
       Value   Percentile   TotalCount 1/(1-Percentile)

       0.097     0.000000            1         1.00
       0.525     0.100000        10042         1.11
...
     172.799     0.999991        99963    109226.67
     172.799     1.000000        99963          inf
#[Mean    =        4.514, StdDeviation   =       16.441]
#[Max     =      172.672, Total count    =        99963]
#[Buckets =           27, SubBuckets     =         2048]
----------------------------------------------------------
  149987 requests in 30.00s, 9.87MB read
  Non-2xx or 3xx responses: 149987
Requests/sec:   4999.96
Transfer/sec:    336.91KB

GET replicas
Running 30s test @ http://localhost:8080
  4 threads and 4 connections
  Thread calibration: mean lat.: 1.570ms, rate sampling interval: 10ms
  Thread calibration: mean lat.: 1.571ms, rate sampling interval: 10ms
  Thread calibration: mean lat.: 1.574ms, rate sampling interval: 10ms
  Thread calibration: mean lat.: 1.593ms, rate sampling interval: 10ms
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency     1.33ms    1.19ms  30.38ms   97.92%
    Req/Sec     1.32k   158.32     4.78k    85.26%
  Latency Distribution (HdrHistogram - Recorded Latency)
 50.000%    1.23ms
 75.000%    1.68ms
 90.000%    2.07ms
 99.000%    2.82ms
 99.900%   20.96ms
 99.990%   28.40ms
 99.999%   29.76ms
100.000%   30.40ms

  Detailed Percentile spectrum:
       Value   Percentile   TotalCount 1/(1-Percentile)

       0.100     0.000000            1         1.00
       0.507     0.100000        10011         1.11
...
      30.399     0.999991        99959    109226.67
      30.399     1.000000        99959          inf
#[Mean    =        1.335, StdDeviation   =        1.186]
#[Max     =       30.384, Total count    =        99959]
#[Buckets =           27, SubBuckets     =         2048]
----------------------------------------------------------
  149984 requests in 30.00s, 9.87MB read
  Non-2xx or 3xx responses: 149984
Requests/sec:   4999.85
Transfer/sec:    336.90KB

______________________________________________
PUT service
Running 30s test @ http://localhost:8080
  4 threads and 4 connections
  Thread calibration: mean lat.: 3152.254ms, rate sampling interval: 10813ms
  Thread calibration: mean lat.: 3318.729ms, rate sampling interval: 11313ms
  Thread calibration: mean lat.: 3501.710ms, rate sampling interval: 11960ms
  Thread calibration: mean lat.: 3335.692ms, rate sampling interval: 11517ms
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency     9.04s     1.62s   12.22s    58.45%
    Req/Sec     5.72k    95.56     5.86k    50.00%
  Latency Distribution (HdrHistogram - Recorded Latency)
 50.000%    8.85s 
 75.000%   10.36s 
 90.000%   11.49s 
 99.000%   12.12s 
 99.900%   12.21s 
 99.990%   12.23s 
 99.999%   12.23s 
100.000%   12.23s 

  Detailed Percentile spectrum:
       Value   Percentile   TotalCount 1/(1-Percentile)

    6217.727     0.000000            5         1.00
    7057.407     0.100000        44742         1.11
...
   12230.655     0.999756       447061      4096.00
   12230.655     1.000000       447061          inf
#[Mean    =     9042.434, StdDeviation   =     1621.191]
#[Max     =    12222.464, Total count    =       447061]
#[Buckets =           27, SubBuckets     =         2048]
----------------------------------------------------------
  555605 requests in 30.00s, 35.50MB read
Requests/sec:  18522.31
Transfer/sec:      1.18MB

PUT Async Service
Running 30s test @ http://localhost:8080
  4 threads and 4 connections
  Thread calibration: mean lat.: 2413.554ms, rate sampling interval: 6991ms
  Thread calibration: mean lat.: 2312.697ms, rate sampling interval: 6746ms
  Thread calibration: mean lat.: 2215.106ms, rate sampling interval: 6512ms
  Thread calibration: mean lat.: 2199.910ms, rate sampling interval: 6488ms
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency    10.29s     5.15s   18.69s    52.19%
    Req/Sec     1.87k   472.16     2.47k    30.00%
  Latency Distribution (HdrHistogram - Recorded Latency)
 50.000%    9.82s 
 75.000%   15.20s 
 90.000%   17.56s 
 99.000%   18.61s 
 99.900%   18.69s 
 99.990%   18.71s 
 99.999%   18.71s 
100.000%   18.71s 

  Detailed Percentile spectrum:
       Value   Percentile   TotalCount 1/(1-Percentile)

    3440.639     0.000000           54         1.00
    3715.071     0.100000        15497         1.11
...
   18710.527     0.999878       154678      8192.00
   18710.527     1.000000       154678          inf
#[Mean    =    10292.113, StdDeviation   =     5150.796]
#[Max     =    18694.144, Total count    =       154678]
#[Buckets =           27, SubBuckets     =         2048]
----------------------------------------------------------
  351844 requests in 30.00s, 22.48MB read
Requests/sec:  11728.97
Transfer/sec:    767.42KB

PUT sharding
Running 30s test @ http://localhost:8080
  4 threads and 4 connections
  Thread calibration: mean lat.: 2.457ms, rate sampling interval: 10ms
  Thread calibration: mean lat.: 2.203ms, rate sampling interval: 10ms
  Thread calibration: mean lat.: 2.331ms, rate sampling interval: 10ms
  Thread calibration: mean lat.: 2.232ms, rate sampling interval: 10ms
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency     1.25ms  843.92us  20.32ms   87.70%
    Req/Sec     1.32k   135.09     3.22k    86.17%
  Latency Distribution (HdrHistogram - Recorded Latency)
 50.000%    1.19ms
 75.000%    1.58ms
 90.000%    1.95ms
 99.000%    2.66ms
 99.900%   14.05ms
 99.990%   18.99ms
 99.999%   19.82ms
100.000%   20.33ms

  Detailed Percentile spectrum:
       Value   Percentile   TotalCount 1/(1-Percentile)

       0.138     0.000000            1         1.00
       0.503     0.100000        10006         1.11
...
      20.335     0.999991        99954    109226.67
      20.335     1.000000        99954          inf
#[Mean    =        1.254, StdDeviation   =        0.844]
#[Max     =       20.320, Total count    =        99954]
#[Buckets =           27, SubBuckets     =         2048]
----------------------------------------------------------
  149976 requests in 30.00s, 9.58MB read
Requests/sec:   4999.71
Transfer/sec:    327.13KB

PUT replicas
Running 30s test @ http://localhost:8080
  4 threads and 4 connections
  Thread calibration: mean lat.: 1.290ms, rate sampling interval: 10ms
  Thread calibration: mean lat.: 1.286ms, rate sampling interval: 10ms
  Thread calibration: mean lat.: 1.281ms, rate sampling interval: 10ms
  Thread calibration: mean lat.: 1.327ms, rate sampling interval: 10ms
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency     1.33ms    1.62ms  38.37ms   99.15%
    Req/Sec     1.32k   173.38     4.78k    86.43%
  Latency Distribution (HdrHistogram - Recorded Latency)
 50.000%    1.20ms
 75.000%    1.59ms
 90.000%    1.97ms
 99.000%    2.76ms
 99.900%   27.15ms
 99.990%   37.25ms
 99.999%   38.17ms
100.000%   38.40ms

  Detailed Percentile spectrum:
       Value   Percentile   TotalCount 1/(1-Percentile)

       0.128     0.000000            1         1.00
       0.505     0.100000        10021         1.11
...
      38.399     0.999991        99969    109226.67
      38.399     1.000000        99969          inf
#[Mean    =        1.329, StdDeviation   =        1.620]
#[Max     =       38.368, Total count    =        99969]
#[Buckets =           27, SubBuckets     =         2048]
----------------------------------------------------------
  149990 requests in 30.00s, 9.58MB read
Requests/sec:   4999.77
Transfer/sec:    327.13KB

Сравнение Service & AsyncService.
Из тестирования производительности HTTP сервера видно, что асинхронная реализация нашего сервера на запросах GET
работает в два раза быстрее, а на PUT - становится чуть лучше. Из профилирования видно, что теперь появились новые
потоки 'worker'-ы.
PUT стал значительно быстрее, GET незначительно.

Сравнение AsyncService & ShardingService
Время обработки запросов для метода PUT немного увеличилась для персентиля 100.0%. Это может объясняеться тем,
что некоторое время тратится теперь на проксирование.
Время вызов STATUS должно было остаться прежним т.к. его обрабатывают непосредственно селекторы сервера,
небольшие изменения во времени - "погрешность машины".
На результатах профилирования видно, что также изменилась и картина cpu профиля для запросов.
Потому что мы добавили новые узлы для обработки запросов.

Сравнение ShardingService & ReplicasService
PUT стал медленнее - ожидаемо
GET быстрее - неожиданно
Данное явление обосновано тем, что в новой реализации с репликами у нас при тестировании параметр replicas был не указан,
а значит использовались дефолтные значения для этого параметра, зависящие от количества нод.
Теперь тратится время на выполнение запросов на нескольких нодах, а также на ожидание получения нескольких ответов.
GET скорее всего стал быстрее, от того что были предыдущие показания сняты неверно.
Картина на Flame Graph-ах значительно не изменилась.
